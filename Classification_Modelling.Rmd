

```{r}
library(tidyverse)
library(tidymodels)
```


# Read in a CSV file
```{r}
df_raw <- read_csv("data/train.csv")
```

# Covert to a dataframe or tibble
# Facorize target variable

```{r}
df <- as_tibble(df_raw) 
df$Cover_Type<- as_factor(df$Cover_Type)
df <- df %>% rename(target = Cover_Type ) %>% select(target, everything())
```




# Rename Target variable - Requires tibble/df as input

## Figure out why this function does not work - Initial idea: Tidyverse function "pull out a different data structure that using $ notation (e.g. df$)


```{r}
rename_target <- function(data, target_variable) {
  data %>% select(target_variable, everything())
  
}


```


# S3 method for workflow
tune_grid(
  object,      # workflow object
  resamples,   # rsamples object
  ...,
  param_info = NULL,  # dials:parameters object
  grid = 10, # dataframe of tuning values
  metrics = NULL,  # yardstick:metric_set()
  control = control_grid() # object for modifying the tuning process
)

# Control_grid object

control_grid(
  verbose = FALSE,
  allow_par = TRUE,
  extract = NULL,
  save_pred = FALSE,
  pkgs = NULL,
  save_workflow = FALSE,
  event_level = "first",
  parallel_over = NULL
)


# S3 method for workflow
tune_bayes(
  object,
  resamples,
  ...,
  iter = 10,
  param_info = NULL,
  metrics = NULL,
  objective = exp_improve(),
  initial = 5,
  control = control_bayes()
)







Create stratified k-fold cross validation 
```{r}
set.seed(23)

split_df <- df %>% initial_split()

train_df <- training(split_df)

test_df <- testing(split_df)

set.seed(21)


#Notes
#This data does not require stratiefied resampling - otherwise add the parameter for strata (default strata = NULL)
# The split into k-folds is only done for the training data set and not for the testing data set

fold_df <- vfold_cv(train_df, v= 5)


```



Pre-processing steps

#Impute
#Handle factor levels
#Individual transformations for skewness and other issues
#Discretize (if needed and if you have no other choice)
#Create dummy variables
#Create interactions
#Normalization steps (center, scale, range, etc)
#Multivariate transformation (e.g. PCA, spatial sign, etc)


```{r}
# no missing 
# The recipe function is applied to the complete training fold and not the k-fold data structure - Check other examples

df_recipe <- recipe(target ~ ., data = train_df) %>% step_nzv(all_predictors()) %>% step_normalize(all_predictors()) %>% step_corr(all_predictors())


```




# Define evaulation metric

```{r}
eval_metric <- metric_set(accuracy)


```


# Define ctrl grids

```{r}
ctrl_grid <- control_grid()
ctrl_res <- control_resamples()
```

# Define models

```{r}

# Linear regression model

lin_reg_mode <-
  linear_reg(
   penalty=tune(),
   mixture=tune()
  ) %>%
  set_engine("lm")

#Regularized linear regression


# Random forest model
rf_mod <- rand_forest(
  mtry = tune(),
  trees = 1000,
  min_n = tune()
) %>%
  set_mode("classification") %>%
  set_engine("ranger")

# Binary regression model

Log_class <- logistic_reg(
  penalty=tune(),
  mixture=tune(),
) %>% set_engine("glmnet")


# XGboost model/Boosted trees

boost_mod <- boost_tree(
  mtry= tune(),
  tree=tune(),
  learn_rate = tune(),
  tree_depth = tune()
  ) %>% set_engine("xgboost") %>%
  set_mode("classification") # Otherwise regression


```



#Define workflow

```{r}
tune_wf <- workflow() %>% add_recipe(df_recipe) %>% add_model(tune_rf)

```

# Fit the model/workflow

```{r}
doParallel::registerDoParallel()

set.seed(135)

tune_res <- tune_grid(
  tune_wf,
  resamples = fold_df,
  metrics = eval_metric,
  grid = 20
  
  
)

tune_res

```











